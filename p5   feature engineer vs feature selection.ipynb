{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering vs feature selection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Feature engineering is to decompose or aggregate raw data to better describe the underlying problem. \n",
    "\n",
    "For algorithms\n",
    "\n",
    "\n",
    " most of our efforts in feature engineering.\n",
    "\n",
    "It goes back to the foundatmental how to ask great question.\n",
    "\n",
    "\n",
    "\n",
    "Feature selection\n",
    "\n",
    "In the real world of data problems, often features are range from the hundreds to thousands.Picking the most important ones to reduces variance of the model aka overfitting, and also the computional cost on training a model. \n",
    "\n",
    "I am going to introduce a tree type of algorithm called random forests that are often used for feature selection. \n",
    "\n",
    "Another type of algorithms for selecting from massive features, is called Lasso regression. Often they are feautes that have more than 100,000+.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "what are the difference in brief:\n",
    "Better features means flexibility.\n",
    "\n",
    "You can choose “the wrong models” (less than optimal) and still get good results. Most models can pick up on good structure in data. The flexibility of good features will allow you to use less complex models that are faster to run, easier to understand and easier to maintain. This is very desirable.\n",
    "\n",
    "Better features means simpler models.\n",
    "\n",
    "With well engineered features, you can choose “the wrong parameters” (less than optimal) and still get good results, for much the same reasons. You do not need to work as hard to pick the right models and the most optimized parameters.\n",
    "\n",
    "With good features, you are closer to the underlying problem and a representation of all the data you have available and could use to best characterize that underlying problem.\n",
    "\n",
    "Better features means better results.\n",
    "\n",
    "The algorithms we used are very standard for Kagglers. […]  We spent most of our efforts in feature engineering.\n",
    "\n",
    "— Xavier Conort, on “Q&A with Xavier Conort” on winning the Flight Quest challenge on Kaggle\n",
    "\n",
    "\n",
    "\n",
    "we learn that sampling the dataset to represent the whole dataset, however what are the most relevant features represent that data and to create solution to your problem?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for four reasons:\n",
    "\n",
    "* simplification of models to make them easier to interpret by researchers/users,[1]\n",
    "* shorter training times,\n",
    "* to avoid the curse of dimensionality ,\n",
    "* enhanced generalization by reducing overfitting[2] (formally, reduction of variance[1])\n",
    "\n",
    "Feature selection is different from dimensionality reduction. Both methods seek to reduce the number of attributes in the dataset, but a dimensionality reduction method do so by creating new combinations of attributes, where as feature selection methods include and exclude attributes present in the data without changing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
